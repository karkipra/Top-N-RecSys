{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Copyright (c) Microsoft Corporation. All rights reserved.</i>\n",
    "\n",
    "<i>Licensed under the MIT License.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running ALS on MovieLens (PySpark)\n",
    "\n",
    "Matrix factorization by [ALS](https://spark.apache.org/docs/latest/api/python/_modules/pyspark/ml/recommendation.html#ALS) (Alternating Least Squares) is a well known collaborative filtering algorithm.\n",
    "\n",
    "This notebook provides an example of how to utilize and evaluate ALS PySpark ML (DataFrame-based API) implementation, meant for large-scale distributed datasets. We use a smaller dataset in this example to run ALS efficiently on multiple cores of a [Data Science Virtual Machine](https://azure.microsoft.com/en-gb/services/virtual-machines/data-science-virtual-machines/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: This notebook requires a PySpark environment to run properly. Please follow the steps in [SETUP.md](https://github.com/Microsoft/Recommenders/blob/master/SETUP.md#dependencies-setup) to install the PySpark environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.7.4 (default, Aug 13 2019, 15:17:50) \n",
      "[Clang 4.0.1 (tags/RELEASE_401/final)]\n",
      "Spark version: 2.4.5\n"
     ]
    }
   ],
   "source": [
    "# set the environment path to find Recommenders\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import time\n",
    "import pyspark\n",
    "from pyspark.ml.recommendation import ALS\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import StringType, FloatType, IntegerType, LongType, ArrayType\n",
    "\n",
    "from reco_utils.dataset import movielens\n",
    "from reco_utils.common.notebook_utils import is_jupyter\n",
    "from reco_utils.dataset.spark_splitters import spark_random_split\n",
    "from reco_utils.evaluation.spark_evaluation import SparkRatingEvaluation, SparkRankingEvaluation\n",
    "from reco_utils.common.spark_utils import start_or_get_spark\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Spark version: {}\".format(pyspark.__version__))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# top k items to recommend\n",
    "TOP_K = 10\n",
    "\n",
    "# Select MovieLens data size: 100k, 1m, 10m, or 20m\n",
    "MOVIELENS_DATA_SIZE = '10k'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Set up Spark context\n",
    "\n",
    "The following settings work well for debugging locally on VM - change when running on a cluster. We set up a giant single executor with many threads and specify memory cap. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following settings work well for debugging locally on VM - change when running on a cluster\n",
    "# set up a giant single executor with many threads and specify memory cap\n",
    "\n",
    "spark = start_or_get_spark(memory=\"16g\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Download the Cell Phone dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define custom schema\n",
    "schema = StructType(\n",
    "    (\n",
    "        StructField(\"asin\", StringType()),\n",
    "        StructField(\"overall\", FloatType()),\n",
    "        StructField(\"reviewerID\", StringType()),\n",
    "        StructField(\"reviewText\", StringType()),\n",
    "        StructField(\"summary\", StringType())\n",
    "    )\n",
    ")\n",
    "\n",
    "# data = movielens.load_spark_df(spark, size=MOVIELENS_DATA_SIZE, schema=schema)\n",
    "data = spark.read.schema(schema).json(\"data/Cell_Phones_and_Accessories.json\").limit(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------+--------------------+--------------------+\n",
      "|      asin|overall|    reviewerID|          reviewText|             summary|\n",
      "+----------+-------+--------------+--------------------+--------------------+\n",
      "|098949232X|    5.0|A1GG51FWU0XQYH|If your into spac...|          Five Stars|\n",
      "|098949232X|    5.0| AVFIDS9RK38E0|   Awesome pictures!|          Five Stars|\n",
      "|098949232X|    5.0|A2S4AVR5SJ7KMI|Great wall art an...|          Five Stars|\n",
      "|098949232X|    5.0| AEMMMVOR9BFLI|As always, it is ...|I love it. I buy ...|\n",
      "|098949232X|    5.0|A2DZXMBTY7KLYP|This is a fantast...|     Great Calendar.|\n",
      "|098949232X|    5.0| AUD367H6I25FX|It's great, I get...|    Awesome Calendar|\n",
      "|098949232X|    5.0|A3K6KUWAZ6SWHE|2015 will be my 3...|     Great calendar!|\n",
      "|098949232X|    5.0|A1FPEO0ME9G4VY|My son loves this...|          Five Stars|\n",
      "|098949232X|    5.0|A20AOY7UXJA710|A great calendar ...|Great Calendar fo...|\n",
      "|098949232X|    5.0|A222LHL23AH0GK|Lots and lots of ...|          Five Stars|\n",
      "|098949232X|    5.0| AZSP9XAX38DG0|I love all the sp...|I love all the sp...|\n",
      "|098949232X|    5.0|A1N5IO8FW9EE6R|Hung it in a high...|          Five Stars|\n",
      "|098949232X|    5.0|A1QI7X0NQ1PUHD|This is a must fo...|A must for astron...|\n",
      "|098949232X|    5.0|A3SF5P30FK4Y5Y|Nice printing. Lo...|Great & interesti...|\n",
      "|098949232X|    5.0|A3KZZYLU5VT4VJ|Very nice and del...|Nice wall calenda...|\n",
      "|098949232X|    5.0| ALK31UO248EH1|I have been buyin...|It is beautifully...|\n",
      "|098949232X|    5.0| AP5I1RWKMQ6BT|Each page is full...| Visual eye candy...|\n",
      "|098949232X|    5.0|A1TKWOUJB8OE8T|One of the cooles...|All the extra coo...|\n",
      "|098949232X|    5.0|A1VKQUQJHJRA3F|Great pictures an...|          Five Stars|\n",
      "|098949232X|    5.0| A1399ZMDHQ1YH|This thing is Big...|Not all Who Wande...|\n",
      "+----------+-------+--------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------------+--------------------+--------------------+------------+-------------+\n",
      "|      asin|overall|          reviewerID|          reviewText|             summary|productIndex|reviewerIndex|\n",
      "+----------+-------+--------------------+--------------------+--------------------+------------+-------------+\n",
      "|B00009WCAP|    1.0|A0265436JMR91F9LHBXT|This did not help...|This product is h...|        5748|            1|\n",
      "|B0002SYC5O|    1.0|A0564474RQMYYH3H95UC|they will not sta...|                Crap|        9151|            2|\n",
      "|8199900164|    5.0|A0617213KGAVUMXH6NK4|Not only does thi...|Efficient and fai...|         966|            3|\n",
      "|B0006TIA8Y|    5.0|A0651739GZEV56UR6T54|great product wit...|Best  USB extende...|       13097|            4|\n",
      "|7508492919|    5.0|A0755549VZ3OU6OE9EHO|perfect exactly w...|             SO cute|         301|            5|\n",
      "|B00006JPBY|    4.0|A09781781CO6UDP1IQGW|Should note the s...|          Convenient|        5010|            6|\n",
      "|B0004OPNTA|    1.0|      A1007QNV235TUF|dont work on sams...|            One Star|       11437|            7|\n",
      "|9652676748|    1.0|      A100C9FK1V6VVT|hard plastic; sli...|This will not pro...|        2172|            8|\n",
      "|B0009B0IX4|    5.0|      A100MPD67FD3ID|After checking ou...|Checked out many ...|       16822|            9|\n",
      "|B0007WWAGI|    5.0|      A100MXMK2MBY0D|This is a great h...|       Great Headset|       14702|           10|\n",
      "|B00005UK9X|    5.0|      A100WO06OQR8BQ|Works great. Easy...|        Great Device|        4814|           11|\n",
      "|B00081GX8O|    2.0|      A1011TQUAC56W5|This is the most ...|       Basic headset|       15869|           12|\n",
      "|B00009WCAP|    1.0|      A101ESX5K665S9|       does not work|            One Star|        5748|           13|\n",
      "|B0007WWAGI|    1.0|      A101RUHV4XZYIL|I opened the ear ...|Opened it and it ...|       14702|           14|\n",
      "|9838427853|    5.0|      A101WG4EQ57NPP|Bought it, love i...|            love it!|        3756|           15|\n",
      "|B0006TIA8Y|    5.0|       A102YA37GMWBT|I just got it and...|        Impressive!!|       13097|           16|\n",
      "|8288862993|    4.0|      A103RLAWEHYFHB|Does what it's su...| Why not FIVE stars?|        1083|           17|\n",
      "|9678315173|    2.0|      A103XTS7PCURDJ|So i got my case ...|        Its okay but|        2246|           18|\n",
      "|9707716371|    5.0|      A103ZDEW9V1FLM|     Awesome battery|          Five Stars|        2321|           19|\n",
      "|B00009AKWF|    5.0|      A1040IWUDHDH68|Good for when I p...|        Old purchase|        5309|           20|\n",
      "+----------+-------+--------------------+--------------------+--------------------+------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "#https://stackoverflow.com/questions/48279056/how-to-create-row-index-for-a-spark-dataframe-using-window-partionby\n",
    "w = Window().partitionBy().orderBy(\"asin\")\n",
    "data = data.withColumn('productIndex',F.rank().over(w))\n",
    "\n",
    "w = Window().partitionBy().orderBy(\"reviewerID\")\n",
    "data = data.withColumn('reviewerIndex',F.rank().over(w))\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "import pickle\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Split the data using the Spark random splitter provided in utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+-------------+\n",
      "|overall|productIndex|reviewerIndex|\n",
      "+-------+------------+-------------+\n",
      "|    1.0|        5748|            1|\n",
      "|    1.0|        9151|            2|\n",
      "|    5.0|         966|            3|\n",
      "|    5.0|       13097|            4|\n",
      "|    5.0|         301|            5|\n",
      "|    4.0|        5010|            6|\n",
      "|    1.0|       11437|            7|\n",
      "|    1.0|        2172|            8|\n",
      "|    5.0|       16822|            9|\n",
      "|    5.0|       14702|           10|\n",
      "|    5.0|        4814|           11|\n",
      "|    2.0|       15869|           12|\n",
      "|    1.0|        5748|           13|\n",
      "|    1.0|       14702|           14|\n",
      "|    5.0|        3756|           15|\n",
      "|    5.0|       13097|           16|\n",
      "|    4.0|        1083|           17|\n",
      "|    2.0|        2246|           18|\n",
      "|    5.0|        2321|           19|\n",
      "|    5.0|        5309|           20|\n",
      "+-------+------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dropping unnecessary columns\n",
    "data = data.select([c for c in data.columns if c in ['productIndex','reviewerIndex','overall', 'reviewText', 'summary']])\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N train 15029\n",
      "N test 4971\n"
     ]
    }
   ],
   "source": [
    "train, test = spark_random_split(data, ratio=0.75, seed=420) #;)\n",
    "print (\"N train\", train.cache().count())\n",
    "print (\"N test\", test.cache().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train the ALS model on the training data, and get the top-k recommendations for our testing data\n",
    "\n",
    "To predict movie ratings, we use the rating data in the training set as users' explicit feedback. The hyperparameters used in building the model are referenced from [here](http://mymedialite.net/examples/datasets.html). We do not constrain the latent factors (`nonnegative = False`) in order to allow for both positive and negative preferences towards movies.\n",
    "Timing will vary depending on the machine being used to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = {\n",
    "    \"itemCol\": \"productIndex\",\n",
    "    \"userCol\": \"reviewerIndex\",\n",
    "    \"ratingCol\": \"overall\",\n",
    "}\n",
    "\n",
    "\n",
    "als = ALS(\n",
    "    rank=10,\n",
    "    maxIter=15,\n",
    "    implicitPrefs=False,\n",
    "    regParam=0.05,\n",
    "    coldStartStrategy='drop',\n",
    "#     nonnegative=TRUE,\n",
    "    seed=42,\n",
    "    #TODO figure out what this does\n",
    "    **header\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 7.106967210769653 seconds for training.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model = als.fit(train)\n",
    "train_time = time.time() - start_time\n",
    "print(\"Took {} seconds for training.\".format(train_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the movie recommendation use case, recommending movies that have been rated by the users do not make sense. Therefore, the rated movies are removed from the recommended items.\n",
    "\n",
    "In order to achieve this, we recommend all movies to all users, and then remove the user-movie pairs that exist in the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 61.163707971572876 seconds for prediction.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Get the cross join of all user-item pairs and score them.\n",
    "users = train.select('productIndex').distinct()\n",
    "items = train.select('reviewerIndex').distinct()\n",
    "user_item = users.crossJoin(items)\n",
    "dfs_pred = model.transform(user_item)\n",
    "\n",
    "# Remove seen items.\n",
    "dfs_pred_exclude_train = dfs_pred.alias(\"pred\").join(\n",
    "    train.alias(\"train\"),\n",
    "    (dfs_pred['productIndex'] == train['productIndex']) & (dfs_pred['reviewerIndex'] == train['reviewerIndex']),\n",
    "    how='outer'\n",
    ")\n",
    "\n",
    "top_all = dfs_pred_exclude_train.filter(dfs_pred_exclude_train[\"train.overall\"].isNull()) \\\n",
    "    .select('pred.' + 'productIndex', 'pred.' + 'reviewerIndex', 'pred.' + \"prediction\")\n",
    "\n",
    "# In Spark, transformations are lazy evaluation\n",
    "# Use an action to force execute and measure the test time \n",
    "top_all.cache().count()\n",
    "\n",
    "test_time = time.time() - start_time\n",
    "print(\"Took {} seconds for prediction.\".format(test_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+-----------+\n",
      "|productIndex|reviewerIndex| prediction|\n",
      "+------------+-------------+-----------+\n",
      "|           1|          587|  -3.286294|\n",
      "|           1|         1208| 0.36795348|\n",
      "|           1|         1348| -3.2554672|\n",
      "|           1|         1677|-0.38888377|\n",
      "|           1|         1702| 0.78687894|\n",
      "|           1|         1720|  0.7981349|\n",
      "|           1|         2086| -1.1226474|\n",
      "|           1|         2324|  -3.286294|\n",
      "|           1|         2483|  -3.905129|\n",
      "|           1|         2667| 0.38557625|\n",
      "|           1|         3452|  -3.286294|\n",
      "|           1|         3468| -1.4968638|\n",
      "|           1|         3668|  -3.286294|\n",
      "|           1|         4136|-0.38994172|\n",
      "|           1|         4949|-0.26167196|\n",
      "|           1|         5501| -1.1960809|\n",
      "|           1|         5562| 0.23899707|\n",
      "|           1|         5668| -0.7484319|\n",
      "|           1|         5957|  -3.905129|\n",
      "|           1|         6031| -0.4864055|\n",
      "+------------+-------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_all.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluate how well ALS performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_eval = SparkRankingEvaluation(test, top_all, k = TOP_K, col_user=\"productIndex\", col_item=\"reviewerIndex\", \n",
    "                                    col_rating=\"overall\", col_prediction=\"prediction\", \n",
    "                                    relevancy_method=\"top_k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:\tALS\n",
      "Top K:\t10\n",
      "MAP:\t0.000375\n",
      "NDCG:\t0.000979\n",
      "Precision@K:\t0.000901\n",
      "Recall@K:\t0.000901\n"
     ]
    }
   ],
   "source": [
    "print(\"Model:\\tALS\",\n",
    "      \"Top K:\\t%d\" % rank_eval.k,\n",
    "      \"MAP:\\t%f\" % rank_eval.map_at_k(),\n",
    "      \"NDCG:\\t%f\" % rank_eval.ndcg_at_k(),\n",
    "      \"Precision@K:\\t%f\" % rank_eval.precision_at_k(),\n",
    "      \"Recall@K:\\t%f\" % rank_eval.recall_at_k(), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluate rating prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+-------------+-----------+\n",
      "|overall|productIndex|reviewerIndex| prediction|\n",
      "+-------+------------+-------------+-----------+\n",
      "|    5.0|       16224|        10377|  4.9629183|\n",
      "|    3.0|       16224|         8061| -2.7615619|\n",
      "|    4.0|        5529|          631|-0.20651442|\n",
      "|    4.0|        5529|        17054| 0.08290452|\n",
      "|    4.0|        5529|         2132|  -1.548662|\n",
      "|    4.0|       14410|        14817| 0.50653875|\n",
      "|    2.0|       14410|         4883| 0.20261544|\n",
      "|    4.0|        8053|         1876|   2.069514|\n",
      "|    3.0|        8053|        10628|-0.63113046|\n",
      "|    1.0|        8053|         5745|  1.9729208|\n",
      "|    3.0|       14370|         7448|  4.1374593|\n",
      "|    4.0|        4227|        11382| -2.1725307|\n",
      "|    4.0|       11437|         6912|  2.0627642|\n",
      "|    2.0|       11437|        10047|  1.3751763|\n",
      "|    4.0|       13988|         2927|   3.784785|\n",
      "|    5.0|        8131|         9362|  1.2370882|\n",
      "|    3.0|       16543|         2142|  3.9550729|\n",
      "|    1.0|       16543|        13528|  0.9887682|\n",
      "|    4.0|       16543|         1259|   2.418448|\n",
      "|    5.0|       16543|         6667|  3.1583385|\n",
      "+-------+------------+-------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate predicted ratings.\n",
    "prediction = model.transform(test)\n",
    "prediction.cache().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:\tALS rating prediction\n",
      "RMSE:\t3.371438\n",
      "MAE:\t2.827619\n",
      "Explained variance:\t-1.371053\n",
      "R squared:\t-5.381247\n"
     ]
    }
   ],
   "source": [
    "rating_eval = SparkRatingEvaluation(test, prediction, col_user=\"productIndex\", col_item=\"reviewerIndex\", \n",
    "                                    col_rating=\"overall\", col_prediction=\"prediction\")\n",
    "\n",
    "print(\"Model:\\tALS rating prediction\",\n",
    "      \"RMSE:\\t%f\" % rating_eval.rmse(),\n",
    "      \"MAE:\\t%f\" % rating_eval.mae(),\n",
    "      \"Explained variance:\\t%f\" % rating_eval.exp_var(),\n",
    "      \"R squared:\\t%f\" % rating_eval.rsquared(), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'papermill' has no attribute 'record'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-890acb23b3be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Record results with papermill for tests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mpapermill\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mpm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_at_k\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mpm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ndcg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndcg_at_k\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mpm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"precision\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision_at_k\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'papermill' has no attribute 'record'"
     ]
    }
   ],
   "source": [
    "if is_jupyter():\n",
    "    # Record results with papermill for tests\n",
    "    import papermill as pm\n",
    "    pm.record(\"map\", rank_eval.map_at_k())\n",
    "    pm.record(\"ndcg\", rank_eval.ndcg_at_k())\n",
    "    pm.record(\"precision\", rank_eval.precision_at_k())\n",
    "    pm.record(\"recall\", rank_eval.recall_at_k())\n",
    "    pm.record(\"rmse\", rating_eval.rmse())\n",
    "    pm.record(\"mae\", rating_eval.mae())\n",
    "    pm.record(\"exp_var\", rating_eval.exp_var())\n",
    "    pm.record(\"rsquared\", rating_eval.rsquared())\n",
    "    pm.record(\"train_time\", train_time)\n",
    "    pm.record(\"test_time\", test_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup spark instance\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
